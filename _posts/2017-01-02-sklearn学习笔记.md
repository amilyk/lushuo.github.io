---
 layout: post
title: "sklearn学习笔记（1）"
date: 2017-01-02 
description: "sklearn学习笔记"
tag: Scikit-learn 
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3> Scikit-learn简介</h3>

简称sklearn，一种非常方便的机器学习的包，包含了完善的文档和丰富的机器学习的算法。对于该Python的介绍，我就懒得介绍了，可以自行google。  
sklearn的官方网址在[这](http://scikit-learn.org/) 。     
sklearn还提供了关于机器学习的算法选择路径图，不过这张图仅供参考，并没有展示所有机器学习算法：  
![机器学习算法选择路线图](/images/ml_map.png)

<h3> 拟合模型的参数</h3>

假如拟合完的模型定义为model  
model.coef_：获取各个指标的系数   
model.get_params()：获得模型的参数属性，例如{'copy_X': True, 'fit_intercept': True, 'n_jobs': 1, 'normalize': False}   
model.score:采用R方的方式判断模型拟合进行评分    

<h3>交叉验证</h3>

sklearn提供了交叉验证的函数  
具体代码如下：   

```python
from sklearn import datasets
from sklearn.cross_validation import train_test_split
iris=datasets.load_iris()
iris_X=iris.data
iris_Y=iris.target
x_Train,x_Test,y_Train,y_Test=train_test_split(iris_X,iris_Y,test_size=0.3)
```

将数据集iris的数据按照7:3的比例拆分为训练集和测试集，使用函数 train_test_split将数据集进行拆分。**注意**：返回值的顺序！！！  

<h3>R方</h3>

**R平方**：决定系数，反映因变量的全部变异能通过回归关系被自变量解释的比例。如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少80%  
在统计学中，R平方值的计算方法如下：  
R平方值=回归平方和(ssreg)/总平方和(sstotal)  
其中回归平方和=总平方和-残差平方和(ssresid)  
总平方和：Const参数为True的情况下，总平方和=y的实际值与平均值的平方差之和；Const参数为False的情况下，总平方和=y的实际值的平方和。  
残差平方和：残差平方和=y的估计值与y的实际值的平方差之和。  
 **R方的特点:**   
(1)可决系数是非负的统计量  
(2)可决系数的取值范围：0<=R^2<=1  
(3)可决系数是样本观测值的函数，可决系数R^2是随机抽样而变动的随机变量。为此，对可决系数的统计可靠性也应进行检验。  
<h3> 正则化</h3>  
对于sklearn的正则化有专门的模块--preprocessing  
导入该模块代码：  
```python
from sklearn import preprocessing
```
简单举一个例子，将矩阵进行正则化，如下 ：  
```python
import numpy as np
a=np.array([[10,2.7,3.6],
            [-100,5,-2],
            [120,20,40]],dtype=np.float64)
print(preprocessing.scale(a))
```
输出的结果：  
```python
[[ 0.         -0.85170713 -0.55138018]
 [-1.22474487 -0.55187146 -0.852133  ]
 [ 1.22474487  1.40357859  1.40351318]]
```
**具体操作**  
加载依赖包  
```python
from sklearn import preprocessing #正则化模块
import numpy as np
from sklearn.cross_validation import train_test_split #数据集拆分
from sklearn.datasets.samples_generator import make_classification #样例生成
from sklearn.svm import SVC #SVM模型
import matplotlib.pyplot as plt
```
首先是生成数据（此处仅仅演示正则化，故数据随机生成即可）：  
```python
X,y=make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,
                        random_state=22,n_clusters_per_class=1,scale=100)
```
可以看出生成300个样例，两个特征。
```python
plt.scatter(X[:,0],X[:,1],c=y)
```
绘制出图如下：
![](/images/sklearn/figure_1.png)
可以看出数据的范围是在(-400,400)区间  
将数据拆分成训练集和测试集。  
```python
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
```
采用SVM模型拟合数据  
```python
clf=SVC()
clf.fit(X_train,y_train)
```
输出拟合的结果，R方的形式：  
```python
print(clf.score(X_test,y_test))
```
此时的R方值：**0.466666666667**  
同上面的操作将X进行正则化。  
```python
X=preprocessing.scale(X)#第一种方式
X=preprocessing.minmax_scale(X,feature_range=(-1,1))#第二种方式 默认range(0,1)
```
此时的数据绘制如下图：  
![](/images/sklearn/figure_1-1.png)
数据范如图中所示在(-4,4)区间。  
之后的操作如上面所示，最终的R方值为：**0.933333333333**  
**结果：**经过正则化能够时预测结果很明显的提升！  