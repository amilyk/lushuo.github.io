---
layout: post
title: "sklearn学习笔记（4）"
date: 2017-01-05 
description: "sklearn学习笔记"
tag: Scikit-learn

---


<h3>模型存储</h3>  

有的时候我们需要对训练好的模型进行存储，比如说你花了好多时间精力去训练一个模型，如果没有一个存储模型的机制就很尴尬，难道要重新训练吗？？？答案显然是否嘛，今天刚刚学到了两个模型存储的方法，好，啥也不说了，show me code O(∩_∩)O哈！  
这里模型是采用SVM，数据集依旧是sklearn中的iris。别问为什么，就是喜欢，任性zzZ  

```python
from sklearn.svm import SVC
from sklearn import datasets

clf=SVC()
iris=datasets.load_iris()
X,y=iris.data,iris.target
clf.fit(X,y)
```

这里已经有一个训练好的模型clf，如果我现在不想再接着训练下去，我想存储该模型，那么有两个方法呢。  
<h3>save method 1</h3>  
使用Python自带模块pickle，该模块目的是将数据进行持久化操作。  
调用该模块，并存储模型到本地。  

```Python
import pickle
with open('clf.pickle','wb') as f:
    pickle.dump(clf,f)
```

采用wb（可写，二进制）方式将模型存储到clf.pickle文件中，如果不存在该文件，将会创建该文件。在本地磁盘可以看到已经将该模型存储。  

![](/images/sklearn/4_1.png)
存储起来，想用的时候就需要将该模型读取出来：  

```python
with open('clf.pickle','rb') as f:
    clf2=pickle.load(f)
```

读取出来的模型我重新命名为clf2，测试一下这个clf2好不好使。  我用它来预测前10个，与clf对比一下  

```python
print(clf2.predict(X[0:10]))
print(clf.predict(X[0:10]))
```
结果展示：  

```python
clf2：[0 0 0 0 0 0 0 0 0 0]
clf：[0 0 0 0 0 0 0 0 0 0]
```

可见预测效果是一样的。  
<h3>save method 2</h3>    
第二种方式就是利用sklearn中的包对模型进行数据持久化操作。  

```Python
from sklearn.externals import  joblib
```

使用的模块是joblib，将clf存入到clf.pkl，代码如下    

```python
joblib.dump(clf,'model/clf.pkl',compress=0)
```

参数compress数值的范围为0到9，数值越大表示压缩的程度越高但是读取和写入的时间越慢，比较常用的数字是3，0表示不压缩。    

![](/images/sklearn/4_2.png)
读取模型文件  

```python
clf3=joblib.load('model/clf.pkl')
```

比较一下clf和clf3是否一致  

```python
print(clf3.predict(X[0:10]))
print(clf.predict(X[0:10]))
```

结果显示：

```Python
clf3：[0 0 0 0 0 0 0 0 0 0]
clf：[0 0 0 0 0 0 0 0 0 0]
```

<h3>对比</h3>   
两种方式都可以，但是当处理大数据模型的时候，sklearn官方声称joblib更加高效，但是只能往硬盘上存储。  
